{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f509ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a7ccd",
   "metadata": {},
   "source": [
    "## Linear Equations\n",
    "$$\n",
    "Ax = b\n",
    "$$\n",
    "### Solution 1:\n",
    "- Fixed point iteration:\n",
    "\n",
    "    define $G(x) = Ax - b + x = ( A + I)x - b$, and let $G(x)$ be the x in the next iteration\n",
    "    \n",
    "    The iterative process converges if all eigenvalues of (A+I) is within the unit circle. However, this condition is too restrict, which is unlikely to happen in real cases. Therefore, we use Gauss-Jacobi, Gauss-Seidel and other iterative shemes instead.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646d05d",
   "metadata": {},
   "source": [
    "### Gauss-Jacobi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7ff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_jacobi(A,b, x):\n",
    "    # Extract the diagonal entries\n",
    "    N = np.diagflat(np.diag(A))\n",
    "    P = N - A\n",
    "    return np.linalg.solve(N, b + P.dot(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9d963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully find solutions: [-1.01708116  0.16642876  0.93157338 -0.72527202]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = np.array([[3,1,2,-2],[1,5,2,-3],[2,3,6,3],[1,1,2,-5]])\n",
    "x = np.zeros(shape = len(A))\n",
    "b = np.random.uniform(high = 5,size=len(A))\n",
    "epsilon = 1e-14\n",
    "pre_x = np.zeros_like(x)\n",
    "while True:\n",
    "    pre_x = x\n",
    "    x = gauss_jacobi(A,b,x)\n",
    "    if  ((x - pre_x)**2).sum() < epsilon:\n",
    "        print('Successfully find solutions:', x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e1ba4c",
   "metadata": {},
   "source": [
    "### Gauss-Seidel algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1c8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel(A,b, x,w = 1,shuffle = False):\n",
    "        # Allowing dampening and extrapolating with weight w\n",
    "        # The sequence of solving x's matters for Gauss-Seidel algorithm, we add a shuffle option\n",
    "    if shuffle == True:\n",
    "        values = np.concatenate([A,b.reshape(len(b),-1)],axis=1)\n",
    "        A = values[:,:-1]\n",
    "        b = values[:,-1]\n",
    "    \n",
    "    # Extract the diagonal entries, lower triangular, and upper triangular\n",
    "    L = np.tril(A)\n",
    "    U = A - L\n",
    "        \n",
    "    return w * np.linalg.solve(L, b - U.dot(x)) + (1 - w) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd41053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the two linear system solves into a Linear Solver\n",
    "class LinearSolver:\n",
    "    \n",
    "    # We allow direct call methods in the class\n",
    "    @staticmethod\n",
    "    def gauss_jacobi(A, b, tol=1e-10):\n",
    "        x = np.zeros_like(b)\n",
    "        N = np.diagflat(np.diag(A))\n",
    "        P = A - N\n",
    "        \n",
    "        while True:\n",
    "            pre_x = x.copy()\n",
    "            x = np.linalg.solve(N, b - P.dot(pre_x))\n",
    "            if np.sum((x - pre_x) ** 2) < tol:\n",
    "                print('Successfully found solutions with Jacobi method:', x)\n",
    "                break\n",
    "        \n",
    "        print(\"Results for Ax - b =\", A.dot(x) - b)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss_seidel(A, b, w=1, shuffle=False, tol=1e-10):\n",
    "        x = np.zeros_like(b)\n",
    "        \n",
    "        if shuffle:\n",
    "            indices = np.arange(len(b))\n",
    "            np.random.shuffle(indices)\n",
    "            A = A[indices]\n",
    "            b = b[indices]\n",
    "        \n",
    "        L = np.tril(A)\n",
    "        U = A - L\n",
    "        \n",
    "        while True:\n",
    "            pre_x = x.copy()\n",
    "            x = w * np.linalg.solve(L, b - U.dot(pre_x)) + (1 - w) * pre_x\n",
    "            if np.sum((x - pre_x) ** 2) < tol:\n",
    "                print('Successfully found solutions with Gauss-Seidel method:', x)\n",
    "                break\n",
    "        \n",
    "        print(\"Results for Ax - b =\", A.dot(x) - b)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913ad0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,.5,.3],[.6,1,.1],[.2,.4,1]])\n",
    "b = np.array([5.,7.,4.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24bd7748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found solutions with Jacobi method: [1.67155639 5.86510467 1.31964982]\n",
      "Results for Ax - b = [3.67428675e-06 3.48811842e-06 2.97013887e-06]\n",
      "Successfully found solutions with Gauss-Seidel method: [1.67155553 5.86510185 1.31964815]\n",
      "Results for Ax - b = [ 9.00075156e-07 -1.41752707e-08  0.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.67155553, 5.86510185, 1.31964815])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_solver = LinearSolver()\n",
    "lin_solver.gauss_jacobi(A,b)\n",
    "lin_solver.gauss_seidel(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cfc0e0",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdcb8fe",
   "metadata": {},
   "source": [
    "### Polytope\n",
    "- Polytope method can be used for any $R^n$ functions, regardless of continuity property\n",
    "- Polytope can guarantee a solution but cannot guarantee a global minimum (maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "646ffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polytope_algorithm(f, simplex, epsilon):\n",
    "    \"\"\"\n",
    "    Perform the Polytope Algorithm (also known as the Nelder-Mead method).\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): The objective function to minimize.\n",
    "    simplex (numpy array): Initial simplex, an (n+1)x(n) array.\n",
    "    epsilon (float): Stopping rule parameter.\n",
    "    \n",
    "    Returns:\n",
    "    numpy array: The optimized vertex of the simplex.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = simplex.shape[0]\n",
    "    \n",
    "    while True:\n",
    "        # Step 1: Reorder the simplex vertices so that f(x^1) >= f(x^2) >= ... >= f(x^{n+1})\n",
    "        simplex = np.array(sorted(simplex, key=lambda x: f(x), reverse=True))\n",
    "        # Step 2: Find the least i such that f(x^i) > f(y^i) where y^i is the reflection of x^i\n",
    "        for i in range(n):\n",
    "            # We shrink the distance of the reflection\n",
    "            reflected = 1e-3 * np.mean(np.delete(initial_simplex,i,0)) + simplex[i]\n",
    "            # The first element that exceeds the function value of the reflected\n",
    "            if f(simplex[i]) > f(reflected):\n",
    "                simplex[i] = reflected\n",
    "                break\n",
    "        # Step 3: Stopping rule, when the simplex is small enough\n",
    "        if np.sum(abs(simplex[-1] - simplex[0])) < epsilon:\n",
    "            return simplex[-1]  # Return the best vertex\n",
    "        # Step 4: Shrink simplex\n",
    "        simplex = .5 * simplex[-1] + 0.5 * simplex\n",
    "\n",
    "# Example usage:\n",
    "def objective_function(x):\n",
    "    return np.sum((x - np.array([1.3, 0.8,.3]))**2)  # Example: minimize the sum of squares\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "cca16d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized vertex: [1.21677517 0.71625434 0.46697049]\n"
     ]
    }
   ],
   "source": [
    "# For a 3-dimensional problem, simplex (the convex hull) has three vertex\n",
    "initial_simplex = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "\n",
    "result = polytope_algorithm(objective_function, initial_simplex, epsilon=1e-14)\n",
    "print(\"Optimized vertex:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f5918",
   "metadata": {},
   "source": [
    "### Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38b02311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [10. 14.]\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, I show a 2 dimensional case, and I write a BFGS optimizer more explicitly below \n",
    "x = np.array([1.0,5.0])\n",
    "delta = 1e-10\n",
    "# Newton's method\n",
    "while 1:\n",
    "    print('Current (x,y) = ', x[0], x[1])\n",
    "    fx = pi(x)\n",
    "    grads = grad_func(x)\n",
    "    H = hessian(pi)(x)\n",
    "    s = np.linalg.solve(H, -grads.reshape(-1,1)).flatten()\n",
    "    \n",
    "    if (s**2).sum() < epsilon * (1 + (x**2).sum()):\n",
    "        print('Iteration ends')\n",
    "        if (grads**2).sum() > delta  * (1 + np.abs(fx)):\n",
    "            print('Not optimum')\n",
    "        else:\n",
    "            print('reach an optimum')\n",
    "        print('Success')\n",
    "        break\n",
    "    x = x + s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bee61",
   "metadata": {},
   "source": [
    "### Quasi-Newton method\n",
    "#### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e3d7235d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1421773919.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/vq/y8sw0dx95kg8d24dbz5s_kt80000gn/T/ipykernel_40011/1421773919.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    self.delta =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad, hessian\n",
    "\n",
    "class BFGS:\n",
    "    \"\"\"\n",
    "    Description: This an example of a BFGS optimizer. \n",
    "    For faster convergence, I comment out the delta criterion\n",
    "    \"\"\"\n",
    "    def __init__(self, func, x, epsilon=1e-12,delta = 1e-10):\n",
    "        self.func = func\n",
    "        self.x = np.array(x, dtype=float)\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = \n",
    "        self.H = np.eye(len(x))\n",
    "        self.grad_func = grad(func)\n",
    "        self.hessian_func = hessian(func)\n",
    "        \n",
    "    def line_search(self, s, c1=0.6, c2=0.8):\n",
    "        a = 0.5\n",
    "        fx = self.func(self.x)\n",
    "        grads = self.grad_func(self.x)\n",
    "        \n",
    "        while True:\n",
    "            new_x = self.x + a * s\n",
    "            new_grads = self.grad_func(new_x)\n",
    "            new_fx = self.func(new_x)\n",
    "            \n",
    "            if (new_fx <= fx + c1 * a * (grads.dot(s)) and \n",
    "                np.abs(new_grads.dot(s)) < np.abs(c2 * (grads.dot(s)))):\n",
    "                return a, new_x, new_grads,new_fx\n",
    "            a *= 2\n",
    "            if a > 4:\n",
    "                return 4, new_x, new_grads,new_fx\n",
    "\n",
    "    def update_hessian(self, z, y):\n",
    "        H = self.H\n",
    "        term1 = H - (H.dot(z.reshape(-1, 1)).dot(z.reshape(1, -1)).dot(H)) / (z.dot(H).dot(z))\n",
    "        term2 = (y.reshape(-1, 1).dot(y.reshape(1, -1))) / (y.dot(z))\n",
    "        return term1 + term2\n",
    "\n",
    "    def optimize(self):\n",
    "        while True:\n",
    "            fx = self.func(self.x)\n",
    "            grads = self.grad_func(self.x)\n",
    "            H = self.hessian_func(self.x)\n",
    "            s = np.linalg.solve(H, -grads.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            lda, x_new, new_grads,new_fx = self.line_search(s)\n",
    "            z = lda * s\n",
    "            y = new_grads - grads\n",
    "            print('Current x: ',self.x)\n",
    "            self.x = x_new  # Update x to the new position calculated by line_search\n",
    "            self.H = self.update_hessian(z, y)\n",
    "\n",
    "            print('Current Function Value= ', fx)\n",
    "            if (lda < 1e-10) or ((z**2).sum() < self.epsilon * (1 + (self.x**2).sum())):\n",
    "                print('Iteration ends')\n",
    "#                 if (grads**2).sum() > self.delta * (1 + np.abs(fx)):\n",
    "#                     print('Not optimum')\n",
    "#                 else:\n",
    "#                     print('Reached an optimum')\n",
    "                print('Success')\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bc42099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current x:  [0.60585548 0.89715616 0.68671143]\n",
      "Current Function Value=  86.53270360660764\n",
      "Current x:  [4.64470546 6.87792099 5.26457618]\n",
      "Current Function Value=  21.633175901651885\n",
      "Current x:  [6.66413045 9.8683034  7.55350856]\n",
      "Current Function Value=  5.4082939754129695\n",
      "Current x:  [ 7.67384294 11.36349461  8.69797474]\n",
      "Current Function Value=  1.3520734938532455\n",
      "Current x:  [ 8.17869919 12.11109021  9.27020784]\n",
      "Current Function Value=  0.3380183734633103\n",
      "Current x:  [ 8.43112732 12.48488802  9.55632439]\n",
      "Current Function Value=  0.08450459336582758\n",
      "Current x:  [ 8.55734138 12.67178692  9.69938266]\n",
      "Current Function Value=  0.021126148341456895\n",
      "Current x:  [ 8.62044841 12.76523637  9.7709118 ]\n",
      "Current Function Value=  0.0052815370853640945\n",
      "Current x:  [ 8.65200192 12.81196109  9.80667636]\n",
      "Current Function Value=  0.0013203842713409592\n",
      "Current x:  [ 8.66777868 12.83532346  9.82455865]\n",
      "Current Function Value=  0.00033009606783527206\n",
      "Current x:  [ 8.67566706 12.84700464  9.83349979]\n",
      "Current Function Value=  8.252401695878575e-05\n",
      "Current x:  [ 8.67961125 12.85284523  9.83797036]\n",
      "Current Function Value=  2.0631004239704504e-05\n",
      "Current x:  [ 8.68158334 12.85576552  9.84020565]\n",
      "Current Function Value=  5.15775105993016e-06\n",
      "Current x:  [ 8.68256939 12.85722567  9.84132329]\n",
      "Current Function Value=  1.28943776498254e-06\n",
      "Current x:  [ 8.68306242 12.85795575  9.84188211]\n",
      "Current Function Value=  3.2235944124462647e-07\n",
      "Current x:  [ 8.68330893 12.85832078  9.84216152]\n",
      "Current Function Value=  8.058986031216517e-08\n",
      "Current x:  [ 8.68343218 12.8585033   9.84230123]\n",
      "Current Function Value=  2.0147465077789154e-08\n",
      "Iteration ends\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Simple Example\n",
    "def fun(x):\n",
    "    x0, x1,x2 = x\n",
    "    u = x0**(.3) * x1**(.2)*x2**(.5)\n",
    "    return (u-10)**2\n",
    "\n",
    "bfgs = BFGS(func= fun, x=np.random.uniform(size=3))\n",
    "bfgs.optimize()\n",
    "\n",
    "## You can find a more useful example in the script ``logit_example'' in repo ``DiscreteChoiceModel_2024Summer'', \n",
    "## where I use BFGS optimizer to estimate the plain logit model using maximum likelihood estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6511f0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
